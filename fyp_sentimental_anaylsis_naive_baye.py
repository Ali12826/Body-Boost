# -*- coding: utf-8 -*-
"""FYP_Sentimental_Anaylsis_Naive_baye.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-tiLJ-DsYoXZiECSL1o-JGCk07MHy6EY
"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import make_pipeline

"""# Important libraries
**import pandas as pd:** Imports the pandas library, commonly used for data manipulation and analysis.  

**from sklearn.feature_extraction.text import CountVectorizer:** Importing CountVectorizer for converting text data into a bag-of-words representation.

**from sklearn.naive_bayes import MultinomialNB:** Importing Multinomial Naive Bayes classifier, suitable for text classification tasks.

**from sklearn.metrics import accuracy_score, classification_report:** Importing evaluation metrics like accuracy score and classification report.

**from sklearn.pipeline import make_pipeline:** Importing a function to create a pipeline for simplifying the workflow.


"""

train_df = pd.read_csv('/content/drive/MyDrive/colab/training.csv')
validation_df = pd.read_csv('/content/drive/MyDrive/colab/validation.csv')
test_df = pd.read_csv('/content/drive/MyDrive/colab/test.csv')
train_data, train_labels = train_df['text'], train_df['label']
validation_data, validation_labels = validation_df['text'], validation_df['label']
test_data, test_labels = test_df['text'], test_df['label']
train_df = train_df.dropna() #Droping rows with missing values
train_df = train_df.drop_duplicates(subset=['text', 'label']) # Removeing any duplicates based on 'text' and 'label'

"""# Dataset Pre-Processing
**Loading Datasets:** Loads training, validation, and test datasets from CSV files located at specific file paths using pandas read_csv function
**Extracting Features and Labels:** Extracts the 'text' column as features and the 'label' column as labels from each dataset, creating separate data and label sets for training, validation, and test datasets.                          
**Handling Missing Values:** Removes rows with missing values (NaN) from the training dataset using the dropna function.
**Handling Duplicate Values:** Removes duplicate rows based on both 'text' and 'label' columns from the training dataset using the drop_duplicates function.
"""

model = make_pipeline(CountVectorizer(), MultinomialNB())
model.fit(train_data, train_labels)
validation_predictions = model.predict(validation_data)
validation_accuracy = accuracy_score(validation_labels, validation_predictions)# Evaluating the model on the validation set
print(f"Validation Accuracy: {validation_accuracy:.2f}")
print("Validation Classification Report:")
print(classification_report(validation_labels, validation_predictions))

"""# Training the model & Validation Acuracy
**Create Pipeline:** involves chaining together a sequence of data processing steps and a machine learning model into a single, unified workflow.              
**Train the Model:** Fits the model to the training data (train_data and train_labels) using the fit method, training the model to recognize patterns in the text.
**Make Predictions on Validation Set:** Applies the trained model to the validation dataset (validation_data) to make predictions on the text data using the predict method.
**Evaluate Model Performance on Validation Set:** Computes the accuracy and generates a classification report by comparing the model's predictions (validation_predictions) with the true labels (validation_labels).
"""

test_predictions = model.predict(test_data)
test_accuracy = accuracy_score(test_labels, test_predictions) # Evaluate the model on the test set
print(f"Test Accuracy: {test_accuracy:.2f}")
print("Test Classification Report:")
print(classification_report(test_labels, test_predictions))

def predict_sentiment(tweet):#trained model to predict the sentiment of new tweets

    result = model.predict([tweet])
    return result[0]
new_tweet = "We have won the match"
predicted_sentiment = predict_sentiment(new_tweet)
print(f"Predicted sentiment: {predicted_sentiment}")

"""# Predicting sentiment of New Tweet & Testing data Accuracy
**Make Predictions on Test Set:** The trained model (model) is used to predict labels for the test dataset (test_data) using the predict method, generating test_predictions.                   
**Evaluate Model on Test Set:** The accuracy of the model on the test set is calculated by comparing the predicted labels (test_predictions) with the true labels (test_labels), and the result is stored in test_accuracy.    
**Predict Sentiment of New Tweet:** A function predict_sentiment(tweet) is defined to predict the sentiment of a new tweet using the trained model.
"""

emotion_labels = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear'}

def predict_sentiment(tweet):
    result = model.predict([tweet])
    sentiment_label = emotion_labels[result[0]]
    return sentiment_label

user_input_tweet = input("Enter a tweet: ")
predicted_sentiment = predict_sentiment(user_input_tweet)
print(f"Predicted sentiment for the entered tweet: {predicted_sentiment}")

"""#Conversion of Emotion Labels
**Emotion Label Mapping:** In this we are mapping emotion_labels from numeric sentiment labels to human-readable emotions: {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear'}.   
**Sentiment Prediction Function:** The predict_sentiment function takes a tweet as input, predicts its sentiment using a trained model (model).
"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from scipy.special import softmax

tweet = ' My Grand Father died yesterday ðŸ˜’ !  '

tweet_words = []

for word in tweet.split(' '):
    if word.startswith('@') and len(word) > 1:
        word = '@user'
    elif word.startswith('http'):
        word = "http"
    tweet_words.append(word)

tweet_proc = " ".join(tweet_words)

#  model
roberta = "cardiffnlp/twitter-roberta-base-sentiment"
model = AutoModelForSequenceClassification.from_pretrained(roberta)
tokenizer = AutoTokenizer.from_pretrained(roberta)

labels = ['Negative', 'Neutral', 'Positive']

# sentiment analysis
encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')
output = model(**encoded_tweet)

scores = output.logits[0].detach().numpy()
softmax_scores = softmax(scores)

for i in range(len(labels)):
    sentiment = labels[i]
    percentage = softmax_scores[i] * 100
    print(f"{sentiment} Percentage: {percentage:.2f}%")

"""# Pr-Trained model Roberto Facebook AI
**Model used:**  RoBERTa-base model  
**Data set:** This model is trained on 154M tweets    
**Last Release Date:** December 2022  
**Tasks:**  Sentiment Analysis,Offensive Language Identification,Emoji Prediction,Irony Detection.    
**F1 (micro):** 0.7169   
**F1 (macro):** 0.5464

"""