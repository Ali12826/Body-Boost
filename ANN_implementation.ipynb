{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load your training, testing, and validation datasets\n",
        "train_df = pd.read_csv('/training.csv')\n",
        "test_df = pd.read_csv('/test.csv')\n",
        "validation_df = pd.read_csv('/validation.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "le = LabelEncoder()\n",
        "train_df['label'] = le.fit_transform(train_df['label'])\n",
        "test_df['label'] = le.transform(test_df['label'])\n",
        "validation_df['label'] = le.transform(validation_df['label'])\n",
        "\n",
        "# Tokenize and pad the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['text'])\n",
        "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
        "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
        "validation_sequences = tokenizer.texts_to_sequences(validation_df['text'])\n",
        "\n",
        "X_train = pad_sequences(train_sequences)\n",
        "X_test = pad_sequences(test_sequences)\n",
        "X_validation = pad_sequences(validation_sequences)\n",
        "\n",
        "y_train = train_df['label']\n",
        "y_test = test_df['label']\n",
        "y_validation = validation_df['label']\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64, mask_zero=True),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification for sentiment analysis\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification for sentiment analysis\n",
        "])\n",
        "...\n",
        "# Tokenize and pad the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['text'])\n",
        "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
        "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
        "validation_sequences = tokenizer.texts_to_sequences(validation_df['text'])\n",
        "\n",
        "# Pad the sequences to a maximum length of 4224\n",
        "X_train = pad_sequences(train_sequences, maxlen=4224)\n",
        "X_test = pad_sequences(test_sequences, maxlen=4224)\n",
        "X_validation = pad_sequences(validation_sequences, maxlen=4224)\n",
        "\n",
        "# ...\n",
        "# Train the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_validation, y_validation))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "# Make predictions on new data\n",
        "new_tweets = [\"I love this product!\", \"This is terrible.\"]\n",
        "new_sequences = tokenizer.texts_to_sequences(new_tweets)\n",
        "X_new = pad_sequences(new_sequences, maxlen=4224)\n",
        "X_new = X_new.reshape((2, 4224))\n",
        "predictions = model.predict(X_new)\n",
        "predicted_labels = [1 if prediction > 0.5 else 0 for prediction in predictions]\n",
        "\n",
        "print(\"Predicted Labels for New Tweets:\")\n",
        "for tweet, label in zip(new_tweets, predicted_labels):\n",
        "    sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
        "    print(f\"Tweet: {tweet}, Predicted Sentiment: {sentiment}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO0fDgaFe_vQ",
        "outputId": "62263eb1-51b9-4c61-8166-32dfeaefc8f4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 914ms/step - accuracy: 0.3341 - loss: -34695124.0000 - val_accuracy: 0.3520 - val_loss: -599593600.0000\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 110ms/step - accuracy: 0.3300 - loss: -586556160.0000\n",
            "Test Accuracy: 0.35\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "Predicted Labels for New Tweets:\n",
            "Tweet: I love this product!, Predicted Sentiment: Positive\n",
            "Tweet: This is terrible., Predicted Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "source": [
        "pip install keras --upgrade"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmjaAWxxifzh",
        "outputId": "610244cf-c047-424d-8d9d-5fbbf5e1e889"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.23.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    }
  ]
}